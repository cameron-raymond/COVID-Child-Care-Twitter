{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining ‚õè\n",
    "\n",
    "**Purpose:** Collect all relevant Tweet's pertaining to the reopening of schools in the COVID-19 pandemic between Jan. 1, 2020 and Sept. 15, 2020.\n",
    "\n",
    "**Pipeline:**\n",
    "1. Connect to Twitter's Search Tweets API, to the `full archive` endpoint\n",
    "2. Go province by province<sup>1</sup> and:\n",
    "    1. Collect all tweets that mention that an education minister\n",
    "    2. Collect all tweets that contain a dedicated list of keywords/hashtags\n",
    "3. Store collection of tweets in Pandas dataframe, and only keep relevant features (data, geocode, text, author, *etc.*)\n",
    "4. Add an extra column that is the cleaned tweet text.\n",
    "5. Save dataframe to CSV\n",
    "6. Solve the pandemic üéä\n",
    "\n",
    "\n",
    "<sup>1</sup> For more information on what tweets are geocoded, see [Twitter's geofiltering guide](https://developer.twitter.com/en/docs/tutorials/filtering-tweets-by-location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grabbing bearer token from OAUTH\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from searchtweets import collect_results, gen_rule_payload, load_credentials, ResultStream\n",
    "\n",
    "premium_search_args = load_credentials(filename=\"../secrets/new_secret.yaml\",yaml_key=\"search_tweets_api\",env_overwrite=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location Filtering Rules\n",
    "\n",
    "**Query Rules:** Each aspect of the query (mentions, keywords, hashtags, geo, etc...) should be encapsulated in their own brackets. Each part of the query, *aside from geo*, only needs one part to be satisfied, so those are all ORed together. Since geo must be satisfied, the rest of the query is put in brackets and geo is appended at the end.\n",
    "\n",
    "**IMPORTANT** This does not work with the `sandbox` API tier so we need to pony up for `premium` first.\n",
    "\n",
    "To collect tweets from province $X$, search for tweets where the account profile has location containing $X$ **OR** geocoded tweets that fall in $X$ \n",
    "\n",
    "Note: the `geo` attribute is deprecated and is ignored accordingly. For geocoded tweets only the `place` attribute will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# has geo AND one of these place markers\n",
    "country = '((has:geo OR has:profile_geo) (place_country:CA OR profile_country:CA))'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword Strategy\n",
    "*TODO: UPDATE*\n",
    "\n",
    "3 conditions that a tweet must satisfy\n",
    "1. It needs to be about the covid-19 pandemic (covid OR covid-19 OR coronavirus OR pandemic OR lockdown)\n",
    "2. It needs to be about children/parental anxiety (child OR children OR kid OR LO OR toddler OR parent OR family)\n",
    "3. It needs to be about school/the back to school season (school OR risk OR open OR reopen OR safe OR safety OR safely, etc..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(((covid) OR (covid-19) OR (coronavirus) OR (pandemic) OR (lockdown) OR (shutdown) OR (closure) OR (closures) OR (open) OR (reopen) OR (risk) OR (safe) OR (safety) OR (safely)) ((child) OR (children) OR (toddler) OR (toddlers) OR (kid) OR (kids) OR (mom) OR (moms) OR (mother) OR (mothers) OR (dad) OR (dads) OR (father) OR (fathers) OR (parent) OR (parents)) ((school) OR (schools) OR (preschools) OR (preschool) OR (daycare) OR (childcare) OR (class) OR (classroom) OR (classrooms) OR (cohort) OR ((online OR distance OR remote) learning)))'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_filters = [\"covid\",\n",
    "                 \"covid-19\",\n",
    "                 \"coronavirus\",\n",
    "                 \"pandemic\",\n",
    "                 \"lockdown\",\n",
    "                 \"shutdown\",\n",
    "                 \"closure\",\n",
    "                 \"closures\",\n",
    "                 \"open\",\n",
    "                 \"reopen\",\n",
    "                 \"risk\",\n",
    "                 \"safe\",\n",
    "                 \"safety\",\n",
    "                 \"safely\"]\n",
    "\n",
    "covid_filters = \"((\"+\") OR (\".join(covid_filters)+\"))\"\n",
    "\n",
    "school_filters = [\"school\",\n",
    "          \"schools\",\n",
    "          \"preschools\",\n",
    "          \"preschool\",\n",
    "          \"daycare\",\n",
    "          \"childcare\",\n",
    "          \"class\",\n",
    "          \"classroom\",\n",
    "          \"classrooms\",\n",
    "          \"cohort\",\n",
    "          \"(online OR distance OR remote) learning\"]\n",
    "\n",
    "school_filters = \"((\"+\") OR (\".join(school_filters)+\"))\"\n",
    "\n",
    "child_filters = [\"child\",\n",
    "                 \"children\",\n",
    "                 \"toddler\",\n",
    "                 \"toddlers\",\n",
    "                 \"kid\",\n",
    "                 \"kids\",\n",
    "                 \"mom\",\n",
    "                 \"moms\",\n",
    "                 \"mother\",\n",
    "                 \"mothers\",\n",
    "                 \"dad\",\n",
    "                 \"dads\",\n",
    "                 \"father\",\n",
    "                 \"fathers\",\n",
    "                 \"parent\",\n",
    "                 \"parents\"]\n",
    "\n",
    "child_filters = \"((\"+\") OR (\".join(child_filters)+\"))\"\n",
    "\n",
    "keywords = \"(\"+\" \".join([covid_filters,child_filters,school_filters])+\")\"\n",
    "keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(#safeseptember OR #safeseptemberAB OR #safeseptemberBC OR #SafeSeptemberMB OR #safeseptemberNB OR #safeseptemberNL OR #safeseptemberNS OR #safeseptemberON OR #safeseptemberPEI OR #safeseptemberQC OR #safeseptemberSK OR #safeseptemberYT OR #unsafeseptember OR #unsafeseptemberAB OR #unsafeseptemberBC OR #unsafeseptemberMB OR #unsafeseptemberNS OR #unsafeseptemberON OR #unsafeseptemberQC)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtags = [\n",
    "    '#safeseptember',\n",
    "    '#safeseptemberAB',\n",
    "    '#safeseptemberBC',\n",
    "    '#SafeSeptemberMB',\n",
    "    '#safeseptemberNB',\n",
    "    '#safeseptemberNL',\n",
    "    '#safeseptemberNS',\n",
    "    '#safeseptemberON',\n",
    "    '#safeseptemberPEI',\n",
    "    '#safeseptemberQC',\n",
    "    '#safeseptemberSK',\n",
    "    '#safeseptemberYT',\n",
    "    '#unsafeseptember',\n",
    "    '#unsafeseptemberAB',\n",
    "    '#unsafeseptemberBC',\n",
    "    '#unsafeseptemberMB',\n",
    "    '#unsafeseptemberNS',\n",
    "    '#unsafeseptemberON',\n",
    "    '#unsafeseptemberQC',\n",
    "]\n",
    "\n",
    "hashtags = \"(\"+\" OR \".join(hashtags)+\")\"\n",
    "hashtags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Tweets\n",
    "\n",
    "From: \n",
    "* March: 8, 20\n",
    "* April: 8, 20\n",
    "* May: 8, 20\n",
    "* June: 8, 20\n",
    "* July: 8, 20\n",
    "* August: 8, 20\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "def create_query(filters,geo=\"\",lang=\"en\"):\n",
    "    \"\"\"\n",
    "        Takes in a list of fully formed filters that can be satisfied in disjunction.\n",
    "    \"\"\"\n",
    "    lang = f\"lang:{lang}\"\n",
    "    filter_str = \" OR \".join(filters)\n",
    "    query = f\"({filter_str}) {lang} {geo}\"\n",
    "    return query.strip()\n",
    "\n",
    "\n",
    "def return_tweets(query,from_date,to_date,f_name=None,max_pages=1):\n",
    "    name = f\"{from_date}_{to_date}\" if not f_name else f\"{f_name}-{from_date}_{to_date}\"\n",
    "    fp = \"../data/raw_data/{}.json\".format(name)\n",
    "    if os.path.isfile(fp):\n",
    "        with open(fp) as fin:\n",
    "            return json.load(fin),name\n",
    "    print(f\"Making request: {name}\")\n",
    "    rule = gen_rule_payload(query,\n",
    "                        from_date=from_date, #UTC 2018-10-21 00:00\n",
    "                        to_date=to_date,\n",
    "                        results_per_call=500)\n",
    "    rs = ResultStream(rule_payload=rule,\n",
    "                  max_pages=max_pages,\n",
    "                  max_results=10**10,\n",
    "                  **premium_search_args)\n",
    "    tweets = list(rs.stream())\n",
    "    with open(fp, 'w') as fout:\n",
    "        json.dump(tweets,fout,indent=4)\n",
    "    return tweets,name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((((covid) OR (covid-19) OR (coronavirus) OR (pandemic) OR (lockdown) OR (shutdown) OR (closure) OR (closures) OR (open) OR (reopen) OR (risk) OR (safe) OR (safety) OR (safely)) ((child) OR (children) OR (toddler) OR (toddlers) OR (kid) OR (kids) OR (mom) OR (moms) OR (mother) OR (mothers) OR (dad) OR (dads) OR (father) OR (fathers) OR (parent) OR (parents)) ((school) OR (schools) OR (preschools) OR (preschool) OR (daycare) OR (childcare) OR (class) OR (classroom) OR (classrooms) OR (cohort) OR ((online OR distance OR remote) learning))) OR (#safeseptember OR #safeseptemberAB OR #safeseptemberBC OR #SafeSeptemberMB OR #safeseptemberNB OR #safeseptemberNL OR #safeseptemberNS OR #safeseptemberON OR #safeseptemberPEI OR #safeseptemberQC OR #safeseptemberSK OR #safeseptemberYT OR #unsafeseptember OR #unsafeseptemberAB OR #unsafeseptemberBC OR #unsafeseptemberMB OR #unsafeseptemberNS OR #unsafeseptemberON OR #unsafeseptemberQC)) lang:en ((has:geo OR has:profile_geo) (place_country:CA OR profile_country:CA)) 1017\n"
     ]
    }
   ],
   "source": [
    "query = create_query([keywords,hashtags],country)\n",
    "from_date = \"2020-04-15\"\n",
    "to_date = \"2020-05-15\"\n",
    "print(query,len(query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making request: 2020-04-15_2020-05-15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('2020-04-15_2020-05-15', 22498)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets,f_name = return_tweets(query,from_date=from_date,to_date=to_date,max_pages=200)\n",
    "f_name,len(tweets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Tweets\n",
    "\n",
    "Feature constructing, tweet cleaning, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import DTYPE, PARSE_DATES, PROVINCES,CONVERTERS\n",
    "from text_cleaning import clean_text\n",
    "from unidecode import unidecode\n",
    "from math import isnan\n",
    "import re\n",
    "\n",
    "decode = lambda x : unidecode(x) if type(x) is str else x\n",
    "\n",
    "def clean_tweet(text,extended_tweet,retweeted_status=None):\n",
    "    if retweeted_status and type(retweeted_status) is dict:\n",
    "        retweeted_status = dict(retweeted_status)\n",
    "        cleaned = clean_tweet(retweeted_status.get(\"text\"),retweeted_status.get(\"extended_tweet\"))[:-1]\n",
    "        return (*cleaned,True)\n",
    "    if pd.isna(extended_tweet):\n",
    "        return clean_text(text), text, False\n",
    "    to_dict = dict(extended_tweet)\n",
    "    return clean_text(to_dict[\"full_text\"]),to_dict[\"full_text\"], False\n",
    "\n",
    "rex = re.compile(r'<a.*?>(.*?)</a>',re.S|re.M)\n",
    "def clean_source(source):\n",
    "    if source:\n",
    "        match = rex.match(source)\n",
    "        return match.groups()[0].strip()\n",
    "    return np.nan\n",
    "\n",
    "clean_user = lambda x : x[\"screen_name\"] if x[\"screen_name\"] else None\n",
    "\n",
    "def clean_entities(entities):\n",
    "    hashtags = [h[\"text\"] for h in entities[\"hashtags\"]] if entities[\"hashtags\"] else np.nan\n",
    "    urls = [h[\"expanded_url\"] for h in entities[\"urls\"]] if entities[\"urls\"] else np.nan\n",
    "    mentions = [h[\"screen_name\"] for h in entities[\"user_mentions\"]] if entities[\"user_mentions\"] else np.nan\n",
    "    return hashtags,urls,mentions\n",
    "\n",
    "in_province = lambda prov : prov in PROVINCES\n",
    "\n",
    "def check_user(user):\n",
    "    user = dict(user)\n",
    "    if \"derived\" in user and \"locations\" in user[\"derived\"]:\n",
    "        loc = dict(user)[\"derived\"][\"locations\"][0]\n",
    "        long_lat = loc.get(\"geo\").get(\"coordinates\")\n",
    "        city = loc.get(\"locality\",np.nan)\n",
    "        prov = loc.get(\"region\",np.nan)\n",
    "        city, prov = decode(city), decode(prov)\n",
    "        loc_tup = (city, prov,*long_lat)\n",
    "        return loc_tup\n",
    "    return (np.nan,np.nan,np.nan,np.nan)\n",
    "    \n",
    "def clean_location(place,user):\n",
    "    if place:\n",
    "        place = dict(place)\n",
    "        long_lat = place[\"bounding_box\"][\"coordinates\"][0][0]\n",
    "        split = [decode(l.strip()) for l in place[\"full_name\"].split(\",\")]\n",
    "        user_loc = check_user(user)\n",
    "        if len(split) == 2:\n",
    "            return tuple(split+long_lat) if in_province(split[-1]) else user_loc\n",
    "        ## AFAIK the only time there's more than 1 comma in a place field is when the place is labelled 'unorganized'\n",
    "        elif len(split) > 2:\n",
    "            # If the tweet location object is having problems and we can derive a user location, do so.\n",
    "            if not user_loc.count(np.nan) or not in_province(split[-1]):\n",
    "                return user_loc\n",
    "            return (np.nan,split[-1],*long_lat)\n",
    "        else:\n",
    "            # If the tweet location object is having problems and we can derive a user location, do so.\n",
    "            if not user_loc.count(np.nan):\n",
    "                return user_loc\n",
    "            return (split[0],np.nan,*long_lat)\n",
    "    else:\n",
    "        return check_user(user)\n",
    "\n",
    "def JSON_to_CSV(f_name):\n",
    "    clean_fp = \"../data/processed_data/{}.csv\".format(f_name)\n",
    "    ## IF the file csv already exists don't waste time doing all the cleaning again\n",
    "    if os.path.isfile(clean_fp):\n",
    "        return pd.read_csv(clean_fp,\n",
    "                           index_col=0,\n",
    "                           header=0,\n",
    "                           dtype=DTYPE,\n",
    "                           converters=CONVERTERS,\n",
    "                           parse_dates=PARSE_DATES)\n",
    "    # If the zipped file already exists unzip it, write the plain file to the local storage and return the dataframe\n",
    "    if os.path.isfile(clean_fp+\".gz\"):\n",
    "        cov_tweets = pd.read_csv(clean_fp+\".gz\",\n",
    "                                 index_col=0,\n",
    "                                 header=0,\n",
    "                                 compression='gzip',\n",
    "                                 dtype=DTYPE,\n",
    "                                 converters=CONVERTERS,\n",
    "                                 parse_dates=PARSE_DATES)\n",
    "        cov_tweets.to_csv(clean_fp)\n",
    "        return cov_tweets\n",
    "    cov_tweets = pd.DataFrame(tweets)\n",
    "    cov_tweets = cov_tweets[['id','user','created_at', 'source', 'text','extended_tweet','retweeted_status','place','entities','favorite_count', 'retweet_count']].set_index(\"id\")\n",
    "    # Get twitter handle from user\n",
    "    cov_tweets[\"screen_name\"] = cov_tweets[\"user\"].apply(clean_user)\n",
    "    # clean the tweet text\n",
    "    cov_tweets[[\"text\",\"extended_tweet\",\"is_retweet\"]] = cov_tweets[[\"text\",\"extended_tweet\",\"retweeted_status\"]].apply(lambda x: clean_tweet(*x),axis=1,result_type=\"expand\")\n",
    "    cov_tweets = cov_tweets.rename({\"text\": \"clean_text\",\"extended_tweet\":\"original_text\"},axis=1)\n",
    "    # Get the city/province from the location data\n",
    "    cov_tweets[[\"city\",\"province\",\"longitude\",\"latitude\"]] = cov_tweets[[\"place\",\"user\"]].apply(lambda x : clean_location(*x),axis=1,result_type=\"expand\")\n",
    "    cov_tweets = cov_tweets.drop([\"place\",\"user\"],axis=1)\n",
    "    Through what medium did they post the tweet?\n",
    "    cov_tweets[\"source\"] = cov_tweets[\"source\"].apply(clean_source)\n",
    "    # Extract tweet entities (hashtags, linked urls, etc...)\n",
    "    cov_tweets[[\"hashtags\",\"urls\",\"mentions\"]] = cov_tweets[[\"entities\"]].apply(lambda x : clean_entities(x[\"entities\"]),result_type=\"expand\",axis=1)\n",
    "    cov_tweets = cov_tweets.drop(\"entities\",axis=1)\n",
    "    cov_tweets = cov_tweets[[\"created_at\",\"screen_name\",\"source\",\"clean_text\",\"original_text\",\"is_retweet\",\"favorite_count\",\"retweet_count\",\"hashtags\",\"urls\",\"mentions\",\"city\",\"province\",\"longitude\",\"latitude\"]]\n",
    "    cov_tweets.to_csv(clean_fp+\".gz\",compression='gzip')\n",
    "    cov_tweets.to_csv(clean_fp)\n",
    "    return cov_tweets\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cov_tweets = JSON_to_CSV(f_name)\n",
    "cov_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Politician Mentions\n",
    "Must @ a politician (premier or education minister) and be pertinent to covid AND school reopenings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_minister_dict = {\n",
    "    \"AB\": \"@davideggenAB\",\n",
    "    \"BC\": \"@Rob_Fleming\",\n",
    "    \"MB\": \"@mingoertzen\",\n",
    "    \"NB\": \"@DominicCardy\",\n",
    "    \"NL\": \"@BrianWarr709\",\n",
    "    \"NT\": \"@RJSimpson_NWT\",\n",
    "    \"NS\": \"@zachchurchill\",\n",
    "    \"ON\": \"@Sflecce\",\n",
    "    \"PEI\": \"@bradtrivers\",\n",
    "    \"QC\": \"@jfrobergeQc\",\n",
    "    \"SK\": \"@GordWyant\",\n",
    "    \"YT\": \"@TracyMcPheeRS\"\n",
    "}\n",
    "\n",
    "premier_dict = {\n",
    "    \"AB\": \"@jkenney\",\n",
    "    \"BC\": \"@jjhorgan\",\n",
    "    \"MB\": \"@BrianPallister\",\n",
    "    \"NB\": \"@blainehiggs\",\n",
    "    \"NL\": \"@PremierofNL\",\n",
    "    \"NT\": \"@CCochrane_NWT\",\n",
    "    \"NS\": \"@StephenMcNeil\",\n",
    "    \"NU\": \"@JSavikataaq\",\n",
    "    \"ON\": \"@fordnation\",\n",
    "    \"PEI\": \"@dennyking\",\n",
    "    \"QC\": \"@francoislegault\",\n",
    "    \"SK\": \"@PremierScottMoe\",\n",
    "    \"YT\": \"@Premier_Silver\"\n",
    "}\n",
    "\n",
    "politicians = \" OR \".join([val for _,val in list(premier_dict.items())+list(edu_minister_dict.items())])\n",
    "\n",
    "politicians = f\"(({politicians}) ({covid_filters} {child_filters}))\"\n",
    "query = create_query([politicians],country)\n",
    "from_date = \"2020-08-17\"\n",
    "to_date = \"2020-08-18\"\n",
    "print(query,len(query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
